{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional & Deep Learning : Image classification  on CatsVSDogs dataset.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This tutorial is highly inspired by the [blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) from François Chollet  at the initiative of  [Keras](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Use convolutional networks to  build image classifiers on colour images\n",
    "* Use pre-trained model (VGG/Inception to improve the accuracy of the results)\n",
    "* Fine-Tuned pre-trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Deep Learning Librairies\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.image as kpi\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.backend as k\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.applications as ka\n",
    "\n",
    "# Data visualization\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code lines allow you to check if your computer is using CPU or GPU ressources. <br>\n",
    "**Warning** : You won't be able to use GPU if another notebook is open and still uses GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"GPU\" if \"GPU\" in [k.device_type for k in device_lib.list_local_devices()] else \"CPU\"\n",
    "print(MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used in this TP is the `CatsVSDogs` dataset used in a [Kaggle Contest](https://www.kaggle.com/c/dogs-vs-cats) which contains 25.000 images. It is a huge number when you do not have a lot of computation power. \n",
    "\n",
    "As our goal here is to understand behaviour of algorithms an not to achieve the best performances we have created two different subsambles of this dataset which are available in the *data* directory.\n",
    "\n",
    "* First subsample : 100 cats images and 100 dogs images for training. 40 cats images and 40 dogs images for validation.\n",
    "* Second subsample : 1000 cats images and 1000 dogs images for training. 400 cats images and 400 dogs images for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use some of the image generators of keras, that we will used later, we have to organise the dataset so that each data of a same class are within the same folder. \n",
    "\n",
    "Our data are then organized this way :\n",
    "\n",
    "```\n",
    "data_dir\n",
    "└───subsample/\n",
    "│   └───train/\n",
    "│   │   └───cats/\n",
    "│   │   │   │   cat.0.jpg\n",
    "│   │   │   │   cat.1.jpg\n",
    "│   │   │   │   ...\n",
    "│   │   └───dogs/\n",
    "│   │   │   │   dog.0.jpg\n",
    "│   │   │   │   dog.1.jpg\n",
    "│   │   │   │   ...\n",
    "│   └───validation/\n",
    "│   │   └───cats/\n",
    "│   │   │   │   cat.1000.jpg\n",
    "│   │   │   │   cat.1000.jpg\n",
    "│   │   │   │   ...\n",
    "│   │   └───dogs/\n",
    "│   │   │   │   dog.1000.jpg\n",
    "│   │   │   │   dog.1000.jpg\n",
    "│   │   │   │   ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/' # data path\n",
    "\n",
    "# subsample directory path \n",
    "\n",
    "N_train = 200 #2000 \n",
    "N_val = 80 #800\n",
    "data_dir_sub = data_dir+'subsample_%d_Ntrain_%d_Nval' %(N_train, N_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of the data\n",
    "\n",
    "The `load_img` function allows to load an image as a PIL image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = kpi.load_img(data_dir_sub+'/train/cats/cat.1.jpg')  # this is a PIL image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `img_to_array` generates an `array numpy` from a  PIL image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = kpi.img_to_array(img)  \n",
    "plt.imshow(x/255, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What are the dimensions of the x array? To what correspond these dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "The `ImageDataGenerator` `keras`function allows to apply different treatments on the images (transformation, normalisation). This transformation allows to produce tranformation on the images without saving a lot of changed images on the disk. The transformation are apply *on the fly*. It makes the classifier more robust.\n",
    "\n",
    "All the possible transformations are listed in the documentation of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi.ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = kpi.ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In oder to visualize the produces transformed image, we will use the`.flow()` command that generates transformed images from an original image and saves its in the specified directory.\n",
    "\n",
    "In the following code we produce 8 of these transformed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\")  # this is a PIL image\n",
    "x = kpi.img_to_array(img)  \n",
    "x_ = np.expand_dims(x, axis=0)\n",
    "\n",
    "if not(os.path.isdir(data_dir_sub+\"/preprocessing_example\")):\n",
    "    os.mkdir(data_dir_sub+\"/preprocessing_example\")\n",
    "\n",
    "    i = 0\n",
    "    for batch in datagen.flow(x_, batch_size=1,save_to_dir=data_dir_sub+\"/preprocessing_example\", save_prefix='cat', save_format='jpeg'):\n",
    "        i += 1\n",
    "        if i > 7:\n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list=[]\n",
    "for f in os.listdir(data_dir_sub+\"/preprocessing_example\"):\n",
    "    X_list.append(kpi.img_to_array(kpi.load_img(data_dir_sub+\"/preprocessing_example/\"+f)))\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(16,8))\n",
    "fig.patch.set_alpha(0)\n",
    "ax = fig.add_subplot(3,3,1)\n",
    "ax.imshow(x/255, interpolation=\"nearest\")\n",
    "ax.set_title(\"Image original\")\n",
    "for i,xt in enumerate(X_list):\n",
    "    ax = fig.add_subplot(3,3,i+2)\n",
    "    ax.imshow(xt/255, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Random transformation %d\" %(i+1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cats_transformation.png\", dpi=100, bbox_to_anchor=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Image classification from scratch with a convolutional network\n",
    "\n",
    "We will here build a classifier with a custom architecture of a convolutional network.\n",
    "\n",
    "We first define epochs and batch_size parameters.\n",
    "\n",
    "* `epochs`: we start with a small number (5-10) in order to check that computing time is reasonable.\n",
    "* `batch_size`:When using keras Generator, size of the batch should be a divider of the size of the sample, otherwise algorithms produce very unstable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "We defined two `ImageDataGenerator` objects :\n",
    "\n",
    "* `train_datagen`: for learning, where different transformations are applied as above, in order to pass various examples to the model.\n",
    "* `valid_datagen`: for validation, where only rescaling is applied.\n",
    "\n",
    "**Question** Why do we apply different transformations for learning and validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images have different dimensions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.0.jpg\"))\n",
    "x_1 = kpi.img_to_array(kpi.load_img(data_dir_sub+\"/train/cats/cat.1.jpg\"))\n",
    "x_0.shape, x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is annoying because all images must have the same dimension to be used in this network. \n",
    "\n",
    "The `flow_from_directory` method allows to specify an output dimension in which all transformed images will be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 150\n",
    "img_height = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = kpi.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "valid_datagen = kpi.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/train/\",  # this is the target directory\n",
    "        target_size=(img_width, img_height),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/validation/\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "The model we define is composed of 3 convolution blocks with the following form : \n",
    "\n",
    "* A Conv2D layer with 32-3X3 filters and a `Relu` activation function.\n",
    "* A MaxPooling layer with 2X2 window.\n",
    "\n",
    "Followed by \n",
    "\n",
    "\n",
    "* A flatten layer.\n",
    "* A Dense layer with 64 neurons and a Relu activation function.\n",
    "* A Dropout layer with a 50% drop rate.\n",
    "* A Dense layer with 1 neuron and a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = km.Sequential()\n",
    "model_conv.add(kl.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), data_format=\"channels_last\"))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Conv2D(32, (3, 3)))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Conv2D(64, (3, 3)))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_conv.add(kl.Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model_conv.add(kl.Dense(64))\n",
    "model_conv.add(kl.Activation('relu'))\n",
    "model_conv.add(kl.Dropout(0.5))\n",
    "model_conv.add(kl.Dense(1))\n",
    "model_conv.add(kl.Activation('sigmoid'))\n",
    "\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our problem here is a two classes classifier we will use the `binary_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training can then be done by using the `fit_generator` function instead of the `fit` function used in the MNIST notebook. This function can be used by passing generator object instead of the data to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "model_conv.fit_generator(train_generator, steps_per_epoch=N_train // batch_size, epochs=epochs, \n",
    "                         validation_data=validation_generator,validation_steps=N_val // batch_size)\n",
    "te = time.time()\n",
    "t_learning_conv_simple_model = te-ts\n",
    "print(\"Learning Time for %d epochs : %d seconds\"%(epochs,t_learning_conv_simple_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_conv_val = model_conv.evaluate_generator(validation_generator, N_val /batch_size, verbose=1)\n",
    "score_conv_train = model_conv.evaluate_generator(train_generator, N_train / batch_size, verbose=1)\n",
    "te = time.time()\n",
    "t_prediction_conv_simple_model = te-ts\n",
    "print('Train accuracy:', score_conv_train[1])\n",
    "print('Validation accuracy:', score_conv_val[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_conv_simple_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment the accuracy and loss values for training and validation to the ones observed in the last epochs of training. What do you observe? Is this normal ? \n",
    "\n",
    "**Q** What can you say about the performance of this model?\n",
    "\n",
    "**Exercice** Add more transformation to the learning generator. Does this help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Pre-trained Network\n",
    "\n",
    "We have seen above that the complexity of the data makes it difficult to build quickly an efficient classifier from scratch even  with an elaborate method as a convolutional network.\n",
    "\n",
    "We will now see that this problem can easily be tackled by using **pre-trained models**. These models are models that are very complex (see image below). They have been trained on a very huge amount of image data in order to classify them. \n",
    "\n",
    "The figure below represents a *VGG 16*. This model is composed of *5 convolutional blocks* which allows to build features on the images. The last block is a *fully convolutional block*. This last block can be seen as a simple *MLP model* which is used on the features build by the convolutional block.\n",
    "\n",
    "How this model, designed to solve a different problem that our problem can be helpfull?\n",
    "\n",
    "Here is our two-stage strategy :  \n",
    "1. We will send our data through the 5 convolutional block in order to build features. These block have been trained on a huge amount of data and can then build intelligent features.\n",
    "2. We will build our own MLP classifier designed to solve our CatsVsDogs problem, and we will train it on the features built on the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Network illustration\n",
    "\n",
    "![](https://blog.keras.io/img/imgclf/vgg16_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Build features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download  the weights of the 5 blocks convolutional layer.\n",
    "\n",
    "We will now download the weights of a VGG16 model that has been learned on the [image-net](http://www.image-net.org) dataset. The image-net is composed of millions of images for 1000 categories.\n",
    "\n",
    "If it's the first time you use these weights, you will have to download it (it will start automatically) and they will be save in your home \n",
    "`\"~/.keras/models\"`\n",
    "\n",
    "The *include_top* argument of the `VGG16` application allows to precise if we want to use or not the last block (fully-connected later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet')\n",
    "model_VGG16_without_top.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building features\n",
    "\n",
    "We will now send our data to the loaded model in order to build our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        data_dir_sub+\"/train\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)  \n",
    "features_train = model_VGG16_without_top.predict_generator(generator, N_train / batch_size,  verbose = 1)\n",
    "\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/validation\",\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "features_validation = model_VGG16_without_top.predict_generator(generator, N_val / batch_size,  verbose = 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 :  Building our classifier on top of features\n",
    "\n",
    "We will now build a simple classifier in order to use the previously build features to classify our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "**Exercise** Write this classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classifier_pretrained_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the weights of this classifier to be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_VGG_fcm_val = model_VGG_fcm.evaluate(features_validation, validation_labels)\n",
    "score_VGG_fcm_train = model_VGG_fcm.evaluate(features_train, train_labels)\n",
    "te = time.time()\n",
    "t_prediction_VGG_fcm = te-ts\n",
    "print('Train accuracy:', score_VGG_fcm_train[1])\n",
    "print('Validation accuracy:', score_VGG_fcm_val[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_fcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment the performance of this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG_fcm.save_weights(data_dir_sub+'/weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "We have notably increased the performances of our model with a model that is really quick. We can continue to try to improve our results by modifying the small MLP classifier network we build. \n",
    " \n",
    "But to really improve our model, it would be nice to also change the weights of the previous layers in order to make them fit our problem.\n",
    "This is possible and it's called FineTuning.\n",
    "\n",
    "In this part we will then build a Model which is composed of the 5 convolutional block of the VGG model (with its weights learned on Image Net) and the classifier block we built (with the weights that we have learned previously).\n",
    "\n",
    "![](https://blog.keras.io/img/imgclf/vgg16_modified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation.\n",
    "\n",
    "We first download the model as done previously.\n",
    "\n",
    "However, the model will be trained on our images, we then have to specify the input_shape of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "model_VGG16_without_top = ka.VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a classfier model like the one we built above and we load the learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = km.Sequential()\n",
    "top_model.add(kl.Flatten(input_shape=model_VGG16_without_top.output_shape[1:]))\n",
    "top_model.add(kl.Dense(64, activation='relu'))\n",
    "top_model.add(kl.Dropout(0.5))\n",
    "top_model.add(kl.Dense(1, activation='sigmoid'))\n",
    "\n",
    "top_model.load_weights(data_dir_sub+'/weights_model_VGG_fully_connected_model_%d_epochs_%d_batch_size.h5' %(epochs, batch_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we assemble these two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "model_VGG_LastConv_fcm = km.Model(inputs=model_VGG16_without_top.input, outputs=top_model(model_VGG16_without_top.output))\n",
    "\n",
    "model_VGG_LastConv_fcm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezed block\n",
    "\n",
    "Our model is ready to be fine-tuned! \n",
    "\n",
    "However, as seen above it contains a huge number of parameters that our computer may not handle.\n",
    "\n",
    "We will start by fine-tune only the last block of convolution of our classifier. \n",
    "\n",
    "This is possible by updating the trainable arguments of the layers that we don't want to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_VGG_LastConv_fcm.layers[:15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = kpi.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = kpi.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/train/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    data_dir_sub+\"/validation/\",\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG_LastConv_fcm.compile(loss='binary_crossentropy',\n",
    "              optimizer=ko.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# fine-tune the model\n",
    "ts = time.time()\n",
    "model_VGG_LastConv_fcm.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=N_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=N_val // batch_size)\n",
    "te = time.time()\n",
    "t_learning_VGG_LastConv_fcm = te-ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "score_VGG_LastConv_fcm_val = model_VGG_LastConv_fcm.evaluate_generator(validation_generator, N_val // batch_size, verbose=1)\n",
    "score_VGG_LastConv_fcm_train = model_VGG_LastConv_fcm.evaluate_generator(train_generator, N_train // batch_size, verbose=1)\n",
    "\n",
    "te = time.time()\n",
    "t_prediction_VGG_LastConv_fcm = te-ts\n",
    "print('Train accuracy:', score_VGG_LastConv_fcm_val[1])\n",
    "print('Validation accuracy:', score_VGG_LastConv_fcm_train[1])\n",
    "print(\"Time Prediction: %.2f seconds\" %t_prediction_VGG_LastConv_fcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how our trained model performs on the kaggle real test dataset (data/test)\n",
    "\n",
    "**Exercise** Apply the model to this dataset and display results on a sample to check it performs well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/test_kaggle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Keras has a lot of pre-trained model\n",
    "\n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "\n",
    "Some have a much more complex architecture like `InceptionV3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Restart the TP by using a different pre-trained model and apply the required modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (IA-GPU)",
   "language": "python",
   "name": "ia-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "865px",
    "left": "0px",
    "right": "1587.01px",
    "top": "106px",
    "width": "213px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
