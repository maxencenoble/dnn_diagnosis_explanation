{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional & Deep Learning : Image classification  on MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "In this TP you will learn to : \n",
    "* Write multilayer perceptron and convolutional network with `Keras`and `Tensorflow`\n",
    "* Understand how `convolutional`, `max pooling`, `stride` and `padding`layers work.\n",
    "* Use these models for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow.keras.utils as ku\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.preprocessing.image as k\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code lines allow you to check if your computer is using CPU or GPU ressources. <br>\n",
    "**Warning** : You won't be able to use GPU if another notebook is open and still uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be used in this TP is the [MNIST DataBase](http://yann.lecun.com/exdb/mnist/).<br>\n",
    "It is composed of 70.000 images (60.000 for learning, 10.000 for test) of 28x28 pixels of handwritten digits from 0 to 9.<br>\n",
    "\n",
    "These data are directly available on the `Keras` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "N_train, N_x_pixels, N_y_pixels = X_train.shape\n",
    "N_test = X_test.shape[0]\n",
    "N_classes = len(set(Y_train))\n",
    "\n",
    "print(\"Train data : %d images  (%d/%d pixels)\" %(N_train, N_x_pixels, N_y_pixels))\n",
    "print(\"Test data : %d images  (%d/%d pixels)\" %(N_test, N_x_pixels, N_y_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "fig =plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(X_train[sample_index], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "ax.set_title(\"image label: %d\" % Y_train[sample_index])\n",
    "ax.grid(False)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification with Multi Layer Perceptron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to learn an image classifier with a MLP model with the following architecture.\n",
    "\n",
    "* A Dense layer with 128 neurons and *relu* activation function\n",
    "* A Dropout Layer with 20% drop rate\n",
    "* A Dense layer with 128 neurons and *relu* activation function\n",
    "* A Dropout Layer with 20% drop rate\n",
    "* A Dense layer with 10 neurons (Number of classes ) and *softmax* activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format\n",
    "\n",
    "Some modifications are required on the data to use them with our model. \n",
    "\n",
    "The first layer is a Dense Layer, which handles 1D vectors as an input. We must first reshape the 2D 28x28 images as a 1D $28*28=784$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape((N_train, N_x_pixels*N_y_pixels))/255\n",
    "X_test_flatten = X_test.reshape((N_test, N_x_pixels*N_y_pixels))/255\n",
    "N_dim_flatten = X_train_flatten.shape[1]\n",
    "print(\"Dimensions of flatten train images : %d X %d\" %(X_train_flatten.shape))\n",
    "print(\"Dimensions of flatten test images : %d X %d\" %(X_test_flatten.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du réseau\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, activation='relu', input_shape=(N_dim_flatten,)))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(128, activation='relu'))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(N_classes, activation='softmax'))\n",
    "# Réumé\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** The summary displays the number of pararameters/weigths of the model. Retrieve these values with the formulas seen in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now instantiate your model by defining :\n",
    "* An optimizer : `RMSprop`\n",
    "* a loss function : `Categorical crossentropy`\n",
    "* Metric : This argument is an option, it allows to compute the metric if you want to check the evolution of the training. Here we choose to compute the accuracy during the training.\n",
    "\n",
    "**Note** : In Keras you can choose either \"sparse_categorical_crossentropy\" or \"categorical_crossentropy\" loss. The former handel 1D (NX1) vectors where each entry contains the label of the data, i.e [0,3,5,9,3,4,...]. The latter handle only one-hot encoding of this vector, ie a 2D vectors (NXN_classes) matrices. Keras has a `to_categorical` functions which allows to convert a vector to its one-hot encoding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=ko.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs=10\n",
    "ts = time.time()\n",
    "history = model.fit(X_train_flatten, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_flatten, Y_test))\n",
    "te = time.time()\n",
    "t_train_mpl = te-ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mpl = model.evaluate(X_test_flatten, Y_test, verbose=0)\n",
    "predict_mpl = model.predict(X_test_flatten)\n",
    "print('Test loss:', score_mpl[0])\n",
    "print('Test accuracy:', score_mpl[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_mpl )\n",
    "fig=plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sb.heatmap(pd.DataFrame(confusion_matrix(Y_test, predict_mpl.argmax(1))), annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about these results?\n",
    "\n",
    "**Exercise** Normalize the data in order to have values between 0 and 1 and run again the learning. What can you say about these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "In this part we will use convolution layers to build a convolutional classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data\n",
    "\n",
    "The convolution architecure takes as input images and no not 1D vectors. However, some data formating are still required.\n",
    "\n",
    "A third dimension is required : the `channels` dimension which will allow to describe each pixel. In our case this dimension's size is only 1 because the images are only defined with grey scale. However for colour images, each pixel is coded with several values (Images are generally encoded with 3 values (RGB channels)). \n",
    "\n",
    "Hence, we need to reshape the images from a 28x28 dimension to a 28X28X1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv = np.expand_dims(X_train,axis=-1)\n",
    "X_test_conv = np.expand_dims(X_test,axis=-1)\n",
    "X_train_conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Edge detection\n",
    "\n",
    "We will first check the transformation applied by a convolution layer.\n",
    "\n",
    "In the following code, we define a convolutional network with  only one filter for which we manually define the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_filter = np.array([\n",
    "        [0.2, -0.2, 0],\n",
    "        [0.2, -0.2, 0],\n",
    "        [0.2, -0.2, 0],\n",
    "    ])\n",
    "\n",
    "def my_init_filter(shape, conv_filter = conv_filter, dtype=None, partition_info=None):\n",
    "    xf,yf = conv_filter.shape\n",
    "    array = conv_filter.reshape(xf, yf, 1, 1)\n",
    "    return array\n",
    "my_init_filter(0).shape\n",
    "\n",
    "conv_edge = km.Sequential([\n",
    "    kl.Conv2D(kernel_size=(3,3), filters=1, kernel_initializer=my_init_filter, input_shape=(28, 28, 1))   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Q** Note that in  `my_init_filter` two dimensions have been added to the conv filter. What do these dimensions represent?\n",
    " \n",
    " The following code allows to display the image, the filter and the convoluted image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_conv[0]\n",
    "img_in = np.expand_dims(x, 0)\n",
    "img_out = conv_edge.predict(img_in)\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax0.imshow(img_in[0,:,:,0], cmap=\"binary\")\n",
    "ax0.set_title(\"Image originale\")\n",
    "ax0.grid(False)\n",
    "\n",
    "norm_conv_filter = (conv_filter-conv_filter.min())/conv_filter.max()\n",
    "ax1.imshow(norm_conv_filter.astype(np.uint8), cmap=\"binary\")\n",
    "ax1.set_title(\"Filtre\")\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2.imshow(img_out[0,:,:,0].astype(np.uint8), cmap=\"binary\")\n",
    "ax2.set_title(\"Image Filtre\")\n",
    "ax2.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What do you see? Are the output image coherent according to the designed filter ?\n",
    "\n",
    "**Exercise** Change the code in order to test different filters (to detect horizontal edge, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides and Padding\n",
    "\n",
    "We will now study the effect on `strides` and `padding` arguments on the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filter = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ])\n",
    "\n",
    "def my_init_filter(shape, conv_filter = conv_filter, dtype=None, partition_info = None):\n",
    "    xf,yf = conv_filter.shape\n",
    "    array = conv_filter.reshape(xf, yf, 1, 1)\n",
    "    return array\n",
    "my_init_filter(0).shape\n",
    "\n",
    "conv_sp = km.Sequential([\n",
    "    kl.Conv2D(kernel_size=(3,3), filters=1, kernel_initializer=my_init_filter, input_shape=(28, 28, 1),\n",
    "           strides=2, padding=\"SAME\") ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What is the effect of the convolutional filter defined here ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_in = np.expand_dims(x, 0)\n",
    "img_out = conv_sp.predict(img_in)\n",
    "\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax0.imshow(img_in[0,:,:,0].astype(np.uint8),\n",
    "           cmap=\"binary\");\n",
    "ax0.grid(False)\n",
    "\n",
    "norm_conv_filter = (conv_filter-conv_filter.min())/conv_filter.max()\n",
    "ax1.imshow(norm_conv_filter.astype(np.uint8),\n",
    "           cmap=\"binary\");\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2.imshow(img_out[0,:,:,0].astype(np.uint8),\n",
    "           cmap=\"binary\");\n",
    "ax2.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Check the dimension of the output images. Are they coherent? <br>\n",
    "**Exercise** Change both *stride* and *padding* arguments and understand the effect of these changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Write a similar code than above to check and understand the behaviour of the `max pooling` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/max_pooling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** What are the dimension of the output image? Is this normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Convolutional Network (ConvNet)*\n",
    "\n",
    "We will now build convolutional networks and see the performances on this kind of model on  image classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5\n",
    "\n",
    "We first test the  LeNet5 model, proposed by LeCun et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet5model = km.Sequential()\n",
    "LeNet5model.add(kl.Conv2D(filters = 6, kernel_size = 5, strides = 1, activation = 'tanh',\n",
    "input_shape = (28,28,1)))\n",
    "LeNet5model.add(kl.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "LeNet5model.add(kl.Conv2D(filters = 16, kernel_size = 5,strides = 1, activation = 'tanh'))\n",
    "LeNet5model.add(kl.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "LeNet5model.add(kl.Flatten())\n",
    "LeNet5model.add(kl.Dense(units = 120, activation = 'tanh'))\n",
    "LeNet5model.add(kl.Dense(units = 84, activation = 'tanh'))\n",
    "LeNet5model.add(kl.Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "LeNet5model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Retrieve 'manually' the number of parameters of this model.\n",
    "\n",
    "**Question** What can you say about the total number of parameters compared with the MLP model defined before? Which layer has the highest number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "batch_size=128\n",
    "epochs=30\n",
    "LeNet5model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=ko.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "ts=time.time()\n",
    "LeNet5model.fit(X_train_conv, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_conv, Y_test))\n",
    "te=time.time()\n",
    "t_train_conv = te-ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Why is the training time longer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_conv = LeNet5model.evaluate(X_test_conv, Y_test, verbose=0)\n",
    "predict_conv = LeNet5model.predict(X_test_conv)\n",
    "print('Test loss:', score_conv[0])\n",
    "print('Test accuracy:', score_conv[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_conv )\n",
    "\n",
    "fig=plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sb.heatmap(pd.DataFrame(confusion_matrix(Y_test, predict_conv.argmax(1))), annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex architecture\n",
    "\n",
    "#### Network\n",
    "\n",
    "We will now design a more complex architecture to try to improve the results of the classification :\n",
    "\n",
    "* A Conv2D layer with 32-3X3 filters and the `Relu` activation function.\n",
    "* A Conv2D layer with 642-3X3 filters and the `Relu` activation function.\n",
    "* A MaxPooling layer with a 2X2 window.\n",
    "* A Dropout layer with a 25% drop rate.\n",
    "* A flatten layer.\n",
    "* A Dense layer with 128 neurons  and the `Relu` activation function.\n",
    "* A Dropout layer with a 50% drop rate.\n",
    "* A Dense layer with 10 neurons  and the `softmax` activation function.\n",
    "\n",
    "\n",
    "**Exercise** Define this model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mnist_conv_architecture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_conv = model.evaluate(X_test_conv, Y_test_cat, verbose=0)\n",
    "predict_conv = model.predict(X_test_conv)\n",
    "print('Test loss:', score_conv[0])\n",
    "print('Test accuracy:', score_conv[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_conv )\n",
    "\n",
    "fig=plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax = sb.heatmap(pd.DataFrame(confusion_matrix(Y_test, predict_conv.argmax(1))), annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment the results.\n",
    "\n",
    "**Q** How to improve them?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
