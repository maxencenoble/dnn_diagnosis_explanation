{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance de caractères manuscrits](https://github.com/wikistat/Ateliers-Big-Data/2-MNIST) ([MNIST](http://yann.lecun.com/exdb/mnist/)) avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé \n",
    "Présentation du problème de reconnaissance de caractères manuscrits ([MNIST DataBase](http://yann.lecun.com/exdb/mnist/) à partir d’images numérisées. L’objectif est de comparer les performances (qualité de prévision, temps d'exécution) en fonction de latechnologie, ici Spark et la librairie MLlib, et en fonction de la taille de l'échantillon. a principale limitation de Spark concerne la mémoire; répartie sur chaque noeud  elle est vite insuffisante lorsque des grands modèles (beaoucp d'arbres d'une forêt aléatoire) doivent être archivés sur chacun de ceux-ci.\n",
    "## 1 Introduction\n",
    "### 1.1 Objetif\n",
    "L'objectif général est la construction d'un meilleur modèle de reconnaissance de chiffres manuscrits. Ce problème est ancien (zipcodes) et sert souvent de base pour la comparaison de méthodes et d'algorithmes d'apprentissage. Le site de Yann Le Cun: [MNIST](http://yann.lecun.com/exdb/mnist/) DataBase, est à la source des données étudiées, il décrit précisément le problème et les modes d'acquisition. Il tenait à jour la liste des publications proposant des solutions avec la qualité de prévision obtenue. Ce problème a également été proposé comme sujet d'un concours [Kaggle](https://www.kaggle.com/competitions) mais sur un sous-ensemble des données. \n",
    "\n",
    "De façon très schématique, plusieurs stratégies sont développées dans une vaste littérature sur ces données.  \n",
    "\n",
    "- Utiliser une méthode classique (k-nn, random forest...) sans trop raffiner mais avec des temps d'apprentissage rapide conduit à un taux d'erreur autour de 3\\%.\n",
    "* Ajouter  ou intégrer un pré-traitement des données permettant de recaler les images par des distorsions plus ou moins complexes.\n",
    "* Construire une mesure de distance adaptée au problème, par exemple invariante par rotation, translation, puis l'intégrer dans une technique d'apprentissage classique comme les $k$ plus proches voisins.\n",
    "* Utiliser une méthode plus flexibles (réseau de neurones épais) avec une optimisation fine des paramètres.\n",
    "\n",
    "L'objectif de cet atelier est de comparer sur des données relativement volumineuses les performances de différents environnements technologiques et librairies.  Une dernière question est abordée, elle concerne l'influence de la taille de l'échantillon d'apprentissage sur le temps d'exécution ainsi que sur la qualité des prévisions.\n",
    "\n",
    "\n",
    "Analyse des données avec Spark, noter les temps d'exécution, la précision estimée sur l'échantillon test.\n",
    "\n",
    "### 1.2 Lecture des données d'apprentissage et de test\n",
    "Les données peuvent être préalablement téléchargées ou directement lues. Ce sont celles originales du site [MNIST DataBase](http://yann.lecun.com/exdb/mnist/) mais préalablement converties au format .csv, certes plus volumineux mais plus facile à lire. Attention le fichier `mnist_train.zip` présent dans le dépôt est compressé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des packages\n",
    "import time\n",
    "from numpy import array\n",
    "# Répertoire courant ou répertoire accessible de tous les \"workers\" du cluster\n",
    "DATA_PATH=\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et transformation des données au format RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont déjà partagée en une partie apprentissage et une test utilisée pour les comparaisons entre méthodes dans les publications. Ce sont bien les données du site MNIST mais transformée au format .csv pour en faciliter la lecture. \n",
    "\n",
    "Elles doivent être stockées à un emplacement accessibles de tous les noeuds du cluster pour permettre la construction de la base de données réparties (RDD). \n",
    "\n",
    "Dans une utilisation monoposte (*standalone*) de *Spark*, elles sont simplement chargées dans le répertoire courant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des fichiers\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_train.csv\",DATA_PATH+\"mnist_train.csv\")\n",
    "f = urllib.request.urlretrieve(\"https://www.math.univ-toulouse.fr/~besse/Wikistat/data/mnist_test.csv\",DATA_PATH+\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du fichier texte en RDD de valeurs\n",
    "## Données d'apprentissage\n",
    "# Transformation ou étape map de séparation des champs\n",
    "trainRDD = sc.textFile(DATA_PATH+\"mnist_train.csv\").map(lambda l: [float(x) for x in l.split(',')])\n",
    "# Action\n",
    "trainRDD.count() # taille de l'échantillon\n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des données au format DataFrame\n",
    "\n",
    "Pour pouvoir être intérprété par les différentes méthodes de classification de la librairie SparkML, les données doivent être converties en objet DataFrame.\n",
    "\n",
    "Pour plus d'information sur l'utilisation de ces DataFrames, reportez vous aux calepins 1-Intro-PySpark/Cal3-PySpark-SQL.ipynb et 1-Intro-PySpark/Cal4-PySpark-Statelem&Pipeline-SparkML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du de la RDD en DataFrame\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Cette fonction va permettre de transformer chaque ligne de la RDD en une \"Row\" pyspark.sql. \n",
    "\n",
    "def list_to_Row(l):    \n",
    "    #Creation d'un vecteur sparse pour les features\n",
    "    features = Vectors.sparse(784,dict([(i,v) for i,v in enumerate(l[:-1]) if v!=0]))\n",
    "    row = Row(label = l[-1], features= features)\n",
    "    return row\n",
    "\n",
    "trainDF = trainRDD.map(list_to_Row).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de ligne\n",
    "trainDF.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Même chose pour les données de test\n",
    "testRDD = sc.textFile(DATA_PATH+'mnist_test.csv').map(lambda l: [float(x) for x in l.split(',')])\n",
    "testRDD.count() # taille de l'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = testRDD.map(list_to_Row).toDF()\n",
    "testDF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sous-échantillon d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extraction d'un sous-échantillon d'apprentissage pour tester les programmes sur des données plus petites. Itérer cette démarche permet d'étudier l'évolution de l'erreur de prévision en fonction de la taille de l'échantillon d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxEch=0.1 # tester pour des tailles croissantes d'échantillon d'apprentissage\n",
    "(trainData, DropDatal) = trainRDD.randomSplit([tauxEch, 1-tauxEch])\n",
    "trainData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de classification\n",
    "\n",
    "Les méthodes de classifications de la librairie SparkML suivent le même shéma d'utilisation. \n",
    "\n",
    "Il faut dans un premier temps crée un objets **Estimators** pour configurer les paramètres de la méthode.\n",
    "Dans un second temps on réalise l'apprentissage en appliquant la fonction **fit** de l'Estimators sur la DataFrame d'apprentissage. Cette commande créé un objet différent, le **Transformers** qui permettra de réaliser les prédictions. \n",
    "\n",
    "Par défaut les différentes méthodes considère que les noms des colonnes correspondants aux variables et au prédicants du jeux d'apprentissage sont respectivement \"features\" et \"label\". Tandis que les prédictions seront automatiquement assigné à une colonne de nom \"prediction\". \n",
    "Il est conseillé de garder cette terminiologie, mais ces attributs par défaut peuvent être modifié en spécifiant les paramètres  *featuresCol*, *labelCol* et *predictionCol* de chaque méthode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'utilisation pour expliciter la syntaxe mais sans grand intérêt pour ces données qui ne satisfont pas à des frontières de discrimination linéaires. L'algorithme permettant de réaliser une regression logistique multinomial est l'algorithme [*softmax*](https://spark.apache.org/docs/latest/ml-classification-regression.html#multinomial-logistic-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01, fitIntercept=False, tol=0.0001,\n",
    "            family = \"multinomial\", elasticNetParam=0.0) #0 for L2 penalty, 1 for L1 penalty\n",
    "\n",
    "### Génération du modèle\n",
    "model_lr = lr.fit(trainDF)\n",
    " \n",
    "time_end=time.time()\n",
    "time_lrm=(time_end - time_start)\n",
    "print(\"LR prend %d s\" %(time_lrm)) # (104s avec taux=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsRDD = model_lr.transform(testDF)\n",
    "labelsAndPredictions = predictionsRDD.select(\"label\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "nb_test = testDF.count()\n",
    "testErr = 1-nb_good_prediction/nb_test\n",
    "print('Test Error = ' + str(testErr)) # (0.08 avec taux =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LogisticRegressionTrainingSummary provides a summary for a LogisticRegressionModel. Currently, only binary classification is supported. Support for multiclass model summaries will be added in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Arbre binaire de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même chose pour un arbre de discrimination. Comme pour l'implémentation de scikit-learn, les arbres ne peuvent être optimisés par un élagage basé sur une pénalisation de la complexité. Ce paramètre n'est pas présent, seule la profondeur max ou le nombre minimal d'observations par feuille peut contrôler la complexité. Noter l'apparition d'un nouveau paramètre: *maxBins* qui, schématiquement, rend qualitative ordinale à maxBins classes toute variable quantitative.  D'autre part, il n'y a pas de représentation graphique. Cette implémentation d'arbre est issue d'un [projet Google](http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/36296.pdf) pour adapter cet algorithme aux contraintes *mapreduce* de données sous Hadoop. Elle vaut surtout pour permettre de construire une implémentation des forêts aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "dt = DecisionTreeClassifier(impurity='gini',maxDepth=5,maxBins=32, minInstancesPerNode=1,\n",
    "                            minInfoGain=0.0)\n",
    "\n",
    "### Génération du modèle\n",
    "model_dt = dt.fit(trainDF)\n",
    "\n",
    "time_end=time.time()\n",
    "time_dt=(time_end - time_start)\n",
    "print(\"DT takes %d s\" %(time_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsRDD = model_dt.transform(testDF)\n",
    "labelsAndPredictions = predictionsRDD.select(\"label\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "nb_test = testDF.count()\n",
    "testErr = 1-nb_good_prediction/nb_test\n",
    "print('Test Error = ' + str(testErr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les $k$-nn ne sont pas \"scalables\" et donc pas présents. Voici la syntaxe et les paramètres associés à l'algorithme des forêts aléatoires. Parmi ceux \"classiques\" se trouvent *numTrees*, *featureSubsetStrategy*, *impurity*, *maxdepth* et en plus *maxbins* comme pour les arbres. Les valeurs du paramètres *maxDepth* est critique pour la qualité de la prévision. en principe, il n'est pas contraint, un arbre peut se déployer sans \"limite\" mais face à des données massives cela peut provoquer des plantages intempestifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "rf = RandomForestClassifier(numTrees = 2, impurity='gini', maxDepth=12,\n",
    "                            maxBins=32, seed=None)\n",
    "\n",
    "### Génération du modèle\n",
    "model_rf = rf.fit(trainDF)\n",
    "\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsRDD = model_rf.transform(testDF)\n",
    "labelsAndPredictions = predictionsRDD.select(\"label\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "nb_test = testDF.count()\n",
    "testErr = 1-nb_good_prediction/nb_test\n",
    "print('Test Error = ' + str(testErr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même traitement sur la totalité de l'échantillon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Quelques résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "100 arbres, sélection automatique, maxDepth=9\n",
    "\n",
    "maxBins | Temps |  Erreur \n",
    "--------|-------|---------\n",
    "32 | 259 |  0.067 \n",
    "64 | 264 |  0.068 \n",
    "128 | 490 | 0.065\n",
    "\n",
    "100 arbres, sélection automatique, maxBins=32\n",
    "\n",
    "maxDepth | Temps | Erreur\n",
    "---------|-------|-------\n",
    "4 | 55 | 0.21\n",
    "9 | 259 |  0.067\n",
    "18 | 983 | **0.035**\n",
    "\n",
    "Le nombre de variables tirées à chaque noeud n'a pas été optimisé. \n",
    "\n",
    "Le paramètre maxBins ne semble pas trop influencer la précision du modèle, au contriare de la profondeur maximum des arbres. Avec une profondeur suffisante, on retrouve (presque) les résultats classiques des forêts aléatoires sur ces données.\n",
    "\n",
    "COmparer les résultats obtenus pour les trois environnements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "226px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
